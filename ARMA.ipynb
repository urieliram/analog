{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvC9Neb7LtTu"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JboE4Y4YndQq",
        "outputId": "9ca452ba-07a1-4f53-a4e4-97ee22cc7c9d"
      },
      "outputs": [],
      "source": [
        "#!pip install spectrum\n",
        "#!pip install dtw-python\n",
        "#!pip install dtreeviz\n",
        "#!pip install sktime\n",
        "#!pip install pmdarima\n",
        "#!pip install statsmodels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuC_OkW4nm78",
        "outputId": "e941ff0d-2e56-4bd1-ca45-c8f3870ca6df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import timeit\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "#import seaborn as sns\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.tools.eval_measures as bias\n",
        "\n",
        "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from statsmodels.tsa.ar_model import AutoReg\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
        "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, LarsCV, Lasso, Ridge, BayesianRidge, LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error,mean_squared_error,mean_absolute_percentage_error\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, AdaBoostRegressor, BaggingRegressor\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from dtw import *\n",
        "from scipy import stats\n",
        "\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "\n",
        "#%matplotlib inline\n",
        "#sns.set_theme(style=\"white\")\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyRgIpSPswGc"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e612CLOjx1J"
      },
      "outputs": [],
      "source": [
        "#https://drive.google.com/drive/folders/1Gj3XK9kM-lE18uBMe3qrZOGEm8yAI8i9\n",
        "#https://www.codegrepper.com/code-examples/python/how+to+read+csv+file+from+google+drive+on+google+colab+\n",
        "path = 'https://drive.google.com/uc?export=download&id=' \n",
        "URL  = 'https://drive.google.com/file/d/1057_dPk6rIZgXVku8kmZjq3m8WQNkJZb/view?usp=sharing'\n",
        "df = pd.read_csv(path+URL.split('/')[-2],usecols=[0,1,2,3,4,5,6,7],names=['Date','SERIE1','SERIE2','SERIE3','SERIE4','SERIE5','SERIE6','SERIE7'],\n",
        "                 dtype={'Date':str,'SERIE1':float,'SERIE2':float,'SERIE3':float,'SERIE4':float,'SERIE5':float,'SERIE6':float,'SERIE7':float}) #names=['CLVUNI','TYPE','NODE'], usecols=[1,2,3,4,5,6,]\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPW00kCJlbim"
      },
      "source": [
        "### ETL 5 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "8Xn_AmM-dL2C",
        "outputId": "611e730b-0cac-43d5-d54b-6eff35efefed"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df['Date'],format= '%d-%m-%Y %H:%M' ) #.dt.date https://pandas.pydata.org/docs/reference/api/pandas.Series.dt.date.html\n",
        "df = df.set_index('Date')\n",
        "df_5 = df.resample('5T').mean()\n",
        "df_5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 678
        },
        "id": "u7HzIcQz8gyT",
        "outputId": "bea8a3e7-9f87-41b4-9f7d-34751ba949c5"
      },
      "outputs": [],
      "source": [
        "regions = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
        "if  True:\n",
        "  i = 0\n",
        "  j = 0\n",
        "  cmap = plt.cm.Dark2 #tab20 https://matplotlib.org/stable/gallery/color/colormap_reference.html\n",
        "  fig, ax = plt.subplots(figsize=(20,14))\n",
        "  for ts in df_5:\n",
        "    max_t = df_5[ts].max()\n",
        "    plt.plot(df_5[ts].index, df_5[ts] / max_t + j, c = cmap(i), label=regions[i], alpha=.9)\n",
        "    i += 1\n",
        "    j -= 1\n",
        "  plt.yticks([])\n",
        "  plt.xticks(rotation=90, fontsize=16)\n",
        "  plt.legend(bbox_to_anchor=(1, 0.8), prop={'size': 16}, title=\"Regions\", title_fontsize=16)\n",
        "  #plt.savefig('series.pdf', bbox_inches = 'tight')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJdgEsvHkNN"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZTswSAq8cwZ"
      },
      "source": [
        "### Sample selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcNqmPCJP_yS"
      },
      "outputs": [],
      "source": [
        "def trunc(values, decs=0):\n",
        "    return np.trunc(values*10**decs)/(10**decs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vALZjLAUfkzP"
      },
      "outputs": [],
      "source": [
        "def diff(list1,list2):\n",
        "    difference = []\n",
        "    zip_object = zip(list1, list2)\n",
        "    for list1_i, list2_i in zip_object:\n",
        "        difference.append(list1_i-list2_i)\n",
        "    return difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cnF4RJ6no8V"
      },
      "outputs": [],
      "source": [
        "CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; NARA='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"r-\"; MAGE=\"FF00FF\";\n",
        "def print_serie2(serie_,prototipo_,title_,ytitle_,xtitle_,sizex_=8,sizey_=5,namefile_='fig_t16_serie.png'):\n",
        "    fig, ax1 = plt.subplots(figsize=(sizex_,sizey_))\n",
        "    plt.title(title_,fontsize='x-large',color=NARA)\n",
        "    ax1.set_xlabel(xtitle_, color=NARA, fontsize='large')\n",
        "    ax1.set_ylabel(ytitle_, color=NARA, fontsize='large')\n",
        "    plt.tick_params(colors = NARA, which='both')\n",
        "    ax1.spines['bottom'].set_color(NARA)\n",
        "    ax1.spines['top'   ].set_color(NARA) \n",
        "    ax1.spines['right' ].set_color(NARA)\n",
        "    ax1.spines['left'  ].set_color(NARA)\n",
        "    if len(prototipo_) != 0: \n",
        "        plt.plot(prototipo_,alpha=0.6, linestyle='dashed', color='red', linewidth=3)\n",
        "    for p in serie_:\n",
        "        plt.plot(p,alpha=0.3, linewidth=2)    \n",
        "    plt.savefig(namefile_, transparent=True)         \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls5XHfg-e5Gr"
      },
      "outputs": [],
      "source": [
        "def OLSstep(X, Y, X_2, pi_step_=0.001,verbose_=False):\n",
        "    model   = sm.OLS(Y, X)\n",
        "    results = model.fit()\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "\n",
        "    ## We sort the 'pi' values and the largest one is selected.\n",
        "    i = 0\n",
        "    pvalues = []\n",
        "    for pi in results.pvalues:\n",
        "        pvalues.append((i,pi))\n",
        "        i = i + 1\n",
        "    pvalues.sort(key=lambda tup: tup[1], reverse=True) ## We order by 'pi'\n",
        "    (i, pi) = pvalues[0]  \n",
        "\n",
        "    while pi > pi:\n",
        "        X   = sm.add_constant(X)\n",
        "        X_2 = sm.add_constant(X_2)   \n",
        "        if verbose_==True:\n",
        "            print('Retiramos regresor ---> X' + str(i))\n",
        "        X   = np.delete(arr=X,   obj=i+0, axis=1)\n",
        "        X_2 = np.delete(arr=X_2, obj=i+0, axis=1)   \n",
        "        model   = sm.OLS(Y, X)\n",
        "        results = model.fit()\n",
        "\n",
        "        ## We sort the 'pi' values and select the largest\n",
        "        i = 0\n",
        "        pvalues = []\n",
        "        for pi in results.pvalues:\n",
        "            pvalues.append((i,pi))\n",
        "            i = i + 1\n",
        "        pvalues.sort(key=lambda tup: tup[1], reverse=True) ## We order by 'pi'\n",
        "        (i, pi) = pvalues[0]\n",
        "        #prediction   = results.predict(X)  \n",
        "        prediction_Y2 = results.predict(X_2)\n",
        "    if len(prediction_Y2) == 0:      \n",
        "        if verbose_==True:\n",
        "            print('>>> Warning, no variable was significant in the regression.')\n",
        "        model   = sm.OLS(Y, X)\n",
        "        results = model.fit()\n",
        "        prediction_Y2 = results.predict(X_2)\n",
        "        \n",
        "    if verbose_==True:\n",
        "        print(results.summary())\n",
        "    return prediction_Y2   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8tPs-3mU4dA"
      },
      "outputs": [],
      "source": [
        "def RF(X, Y, X_2,labels_,typedist_,verbose_=False):\n",
        "    model         = RandomForestRegressor(random_state=42)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   \n",
        "def Boosting(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = GradientBoostingRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2\n",
        "def Bagging(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = BaggingRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2   \n",
        "def AdaBoost(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = AdaBoostRegressor(random_state=42,)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2 \n",
        "def LinearReg(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = LinearRegression()\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2\n",
        "def BayesRidge(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = BayesianRidge(compute_score=True)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2) \n",
        "    return prediction_Y2\n",
        "def LassoReg(X, Y, X_2,typedist_,verbose_=True):\n",
        "    model         = Lasso(alpha=0.1,max_iter=10000)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2\n",
        "def RidgeReg(X, Y, X_2,typedist_,verbose_=False):\n",
        "    model         = Ridge(alpha=0.1)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2    \n",
        "def PLS(X, Y, X_2,n_components,typedist_,verbose_=False):\n",
        "    model         = PLSRegression(n_components=n_components)\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2\n",
        "def PCR(X, Y, X_2,n_components,typedist_,verbose_=False):\n",
        "## https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html\n",
        "    model         = make_pipeline(PCA(n_components=n_components), LinearRegression())\n",
        "    results       = model.fit(X, Y)\n",
        "    prediction_Y2 = results.predict(X_2)\n",
        "    return prediction_Y2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFQb6V2fYha7"
      },
      "outputs": [],
      "source": [
        "def VotingEnsemble(X, Y, X_2,verbose_=False):  \n",
        "    ## https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py\n",
        "    gb  = GradientBoostingRegressor(random_state=42)\n",
        "    rf  = RandomForestRegressor(random_state=42)\n",
        "    br  = BaggingRegressor(random_state=42)\n",
        "    ab  = AdaBoostRegressor(random_state=42)\n",
        "    gb.fit(X, Y)\n",
        "    rf.fit(X, Y)\n",
        "    br.fit(X, Y)\n",
        "    ab.fit(X, Y)\n",
        "    voting = VotingRegressor([(\"gb\",gb), (\"rf\",rf), (\"br\",br), (\"ab\",ab)]) #\n",
        "    voting.fit(X, Y)\n",
        "    prediction_Y2 = voting.predict(X_2)\n",
        "    return prediction_Y2   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDO3mPUv3wf5"
      },
      "outputs": [],
      "source": [
        "def VotingLinear(X, Y, X_2,verbose_=False):  \n",
        "    ## https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py\n",
        "    pl = PLSRegression(n_components=1)\n",
        "    lr = LinearRegression()\n",
        "    ri = Ridge(alpha=0.1)\n",
        "    la = Lasso(alpha=0.1)    \n",
        "    pc = make_pipeline(PCA(n_components=1), LinearRegression())\n",
        "    pl.fit(X, Y)\n",
        "    lr.fit(X, Y)\n",
        "    ri.fit(X, Y)\n",
        "    la.fit(X, Y)\n",
        "    pc.fit(X, Y)\n",
        "    voting = VotingRegressor([(\"lr\",lr),(\"ri\",ri),(\"la\",la),(\"pc\",pc)]) #,(\"pl\",pl)\n",
        "    voting.fit(X, Y)\n",
        "    prediction_Y2 = voting.predict(X_2)    \n",
        "    return prediction_Y2   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yY6yd0WJBr7f"
      },
      "outputs": [],
      "source": [
        "def euclidean(neig1, neig2):\n",
        "\tdistance = 0.0\n",
        "\tfor i in range(len(neig1)):\n",
        "\t\tdistance += (neig1[i] - neig2[i])**2\n",
        "\treturn sqrt(distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://thispointer.com/python-how-to-append-a-new-row-to-an-existing-csv-file/#:~:text=Open%20our%20csv%20file%20in,in%20the%20associated%20csv%20file\n",
        "from csv import writer\n",
        "def append_list_as_row(file_name, list_of_elem):\n",
        "    # Open file in append mode\n",
        "    with open(file_name, 'a+', newline='') as write_obj:\n",
        "        # Create a writer object from csv module\n",
        "        csv_writer = writer(write_obj)\n",
        "        # Add contents of list as last row in the csv file\n",
        "        csv_writer.writerow(list_of_elem)\n",
        "# a=1234566; b=987\n",
        "# row = [a,b]\n",
        "# append_list_as_row('test.csv',row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TZJr4q_8mTM"
      },
      "source": [
        "### Forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz9CDOHQnx0j"
      },
      "outputs": [],
      "source": [
        "def analogo_knn(serie,vsele,k=10,tol=0.8,n_components=3,typedist='pearson',typereg='OLSstep',verbose=False):\n",
        "# vesele   : Size of the selection window / Tamanio de la ventana de selección\n",
        "# k        : Number of neighbours to search for k / Número de vecinos a buscar k\n",
        "# tol      : Window size tolerance for neighbour selection / Tolerancia de tamaño de ventanas para seleccion de vecinos\n",
        "# typedist : distance measure, 'euclidian' or 'pearson' or 'dtw' / medida de distancia, 'euclidian' o 'pearson' o 'dtw' \n",
        "    t_o = time.time()\n",
        "    n = len(serie) \n",
        "\n",
        "## STEP 1: Selection of the windows with the highest correlation.\n",
        "\n",
        "    ## Calculate the distance between all neighbors.\n",
        "    distances = []\n",
        "    Y = serie[n-vsele:n]           ## latest data\n",
        "    for i in range(n-2*vsele):                              \n",
        "        if  typedist == 'dtw':     ## dynamic time warping\n",
        "            dist = dtw(Y, serie[i:i+vsele]).distance  \n",
        "        elif typedist == 'euclidian':\n",
        "            dist = euclidean(Y,serie[i:i+vsele])\n",
        "        else:\n",
        "            dist = np.corrcoef(Y,serie[i:i+vsele])[1,0]\n",
        "        if dist > 0:\n",
        "            distances.append((i, dist))\n",
        "        \n",
        "    ## We calculate the neighbourhood by distance from smallest to largest and the positions are saved.\n",
        "    if typedist == 'pearson':\n",
        "        ## In the Pearson backwards case, we are interested in the indices with the highest correlation in Pearson backwards ordering.\n",
        "        distances.sort(key=lambda tup: tup[1], reverse=True)\n",
        "    else:\n",
        "        distances.sort(key=lambda tup: tup[1], reverse=False)\n",
        "\n",
        "    neighbors  = []\n",
        "    neighbors2 = []\n",
        "    positions  = []\n",
        "\n",
        "    ## We calculate the k nearest neighbors and save the positions.\n",
        "    i = 0\n",
        "    for pos, dis in distances:\n",
        "        if i==0:      \n",
        "            positions.append(pos)   \n",
        "            neighbors.append(serie[pos:pos+vsele])\n",
        "            neighbors2.append(serie[pos+vsele:pos+2*vsele])  \n",
        "        else:\n",
        "            bandera = True\n",
        "            for p in positions:\n",
        "                 ## if we already had a position in the list that passed the tolerance, we no longer save it\n",
        "                if (abs(pos - p) < tol*vsele):\n",
        "                    bandera = False\n",
        "                    i = i - 1\n",
        "                    break\n",
        "            if bandera == True:\n",
        "                ## save new neighbor\n",
        "                positions.append(pos)   \n",
        "                neighbors.append(serie[pos:pos+vsele])\n",
        "                neighbors2.append(serie[pos+vsele:pos+2*vsele])  \n",
        "                bandera = False\n",
        "        i = i + 1\n",
        "        if i == k:\n",
        "            break\n",
        "    if verbose==True:\n",
        "        print('positions KNN:', positions) ## position of k nearest neighbors\n",
        "\n",
        "    neighbors  = np.array(neighbors)  \n",
        "    neighbors2 = np.array(neighbors2)    \n",
        "    vacia = []\n",
        "    if verbose==True:\n",
        "        print_serie2(neighbors,Y,'Selección con KNN:'+typedist,'demanda','time',8,5,'fig_t16_X_'+typedist+'_'+typereg)\n",
        "\n",
        "    t_sel = time.time() - t_o\n",
        "\n",
        "## STEP 2: Regression between nearest neighbors 'X' and last window 'Y'\n",
        "\n",
        "    ## Define our regressors\n",
        "    X   = (neighbors.T ).tolist()\n",
        "    X_2 = (neighbors2.T).tolist()\n",
        "    Y   = (Y).tolist()\n",
        "    prediction_Y2 = []\n",
        "\n",
        "    ## -- Random forest regression --\n",
        "    if typereg == 'RF':\n",
        "        prediction_Y2 = RF(X,Y,X_2,labels_=positions,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- OLS with Stepwise --\n",
        "    if typereg == 'OLSstep':\n",
        "        prediction_Y2 = OLSstep(X,Y,X_2, pi_step_=0.001,verbose_=verbose)\n",
        "\n",
        "    ## -- Gradiant boosting regression --\n",
        "    if typereg == 'Boosting':\n",
        "        prediction_Y2 = Boosting(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Bagging regression --\n",
        "    if typereg == 'Bagging':\n",
        "        prediction_Y2 = Bagging(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Linear regression --\n",
        "    if typereg == 'LinearReg':\n",
        "        prediction_Y2 = LinearReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Ada boosting --\n",
        "    if typereg == 'AdaBoost':\n",
        "        prediction_Y2 = AdaBoost(X, Y, X_2,typedist_=typedist,verbose_=verbose)  \n",
        "\n",
        "    ## -- Bayesian Ridge --\n",
        "    if typereg == 'BayesRidge':\n",
        "        prediction_Y2 = BayesRidge(X, Y, X_2,typedist_=typedist,verbose_=verbose)  \n",
        "\n",
        "    ## -- Lasso regression --\n",
        "    if typereg == 'LassoReg':\n",
        "        prediction_Y2 = LassoReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)   \n",
        "\n",
        "    ## -- Ridge regression --\n",
        "    if typereg == 'RidgeReg':\n",
        "        prediction_Y2 = RidgeReg(X, Y, X_2,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- PLS Regression --\n",
        "    if typereg == 'PLS':\n",
        "        prediction_Y2 = PLS(X, Y, X_2,n_components=n_components,typedist_=typedist,verbose_=verbose)\n",
        "        prediction_Y2 = prediction_Y2.flatten()\n",
        "\n",
        "    ## -- PCA Regression --\n",
        "    if typereg == 'PCR':\n",
        "        prediction_Y2 = PCR(X, Y, X_2,n_components=n_components,typedist_=typedist,verbose_=verbose)\n",
        "\n",
        "    ## -- Voting regression with ensemble models -- \n",
        "    if typereg == 'VotingEnsemble':\n",
        "        prediction_Y2 = VotingEnsemble(X,Y,X_2,verbose_=verbose)\n",
        "\n",
        "    ## -- Voting regression with linear model -- \n",
        "    if typereg == 'VotingLinear':\n",
        "        prediction_Y2 = VotingLinear(X,Y,X_2,verbose_=verbose)\n",
        "        \n",
        "    ## -- Random forrest regression with GridSearchCV--\n",
        "    #if typereg == 'AutoRF':\n",
        "    #    prediction_Y2 = AutoRF(X, Y, X_2,labels_=positions,typedist_=typedist,verbose_=verbose)        \n",
        "\n",
        "    if verbose==True:\n",
        "        print_serie2(neighbors2,prediction_Y2, 'Forecast - ' + typedist+' - ' + typereg ,'Demand','Time',8,5,'fig_t16_Y2_'+typedist+'_'+typereg)\n",
        "\n",
        "    t_reg = time.time() - t_sel - t_o\n",
        "    fail_=False\n",
        "    if len(prediction_Y2) == 0:\n",
        "        prediction_Y2=[serie[-1]] * vsele\n",
        "        fail_=True\n",
        "        print(\">>> analogo_knn: Forecast not calculated.\")\n",
        "\n",
        "    ## Draw an example of an analogous space with X, X' and Y,Y'\n",
        "    if False:\n",
        "        fig, ax = plt.subplots(figsize=(8,5))\n",
        "        ax.legend(['First line', 'Second line'])\n",
        "        serie1=serie[positions[0]:positions[0]+2*vsele]\n",
        "        serie2=serie[positions[1]:positions[1]+2*vsele]\n",
        "        serie3=serie[positions[2]:positions[2]+2*vsele]\n",
        "        serie4=serie[positions[3]:positions[3]+2*vsele]\n",
        "        serie5=serie[positions[4]:positions[4]+2*vsele]\n",
        "        serie6=serie[positions[5]:positions[5]+2*vsele]\n",
        "        ax.plot(serie1, label='X$_1$')\n",
        "        ax.plot(serie2, label='X$_2$')\n",
        "        ax.plot(serie3, label='X$_3$')\n",
        "        ax.plot(serie4, label='X$_4$')\n",
        "        ax.plot(serie5, label='X$_5$')\n",
        "        ax.plot(serie6,label='X$_6$')\n",
        "        ax.plot(Y,label='Y', linewidth=3, color='r')   \n",
        "        c = np.concatenate((Y,prediction_Y2), axis=0)   \n",
        "        ax.plot(c, label='$Y\\'$', linewidth=3, color='r',linestyle='--')      \n",
        "        plt.legend()\n",
        "        plt.axvline(x = vsele,linestyle='-.')\n",
        "        ax.set(xlabel='time (5 min)', ylabel='demand (MW)') #title='High correlation windows'\n",
        "        ax.grid()\n",
        "\n",
        "        fig.savefig('test'+str(random.randint(1,30000))+'.pdf')\n",
        "        plt.show()\n",
        "\n",
        "    return prediction_Y2, t_sel, t_reg, fail_, positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHBgJTShea-s"
      },
      "outputs": [],
      "source": [
        "## Persistence forecasting\n",
        "def persistence(serie, n=1):\n",
        "  out = [serie[-1] * n]\n",
        "  return(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akyjoUuj8p8w"
      },
      "outputs": [],
      "source": [
        "def sk_autoarima(x, max_p = 2, max_d = 2, max_q = 2, sp = 1, to_predict = 1):\n",
        "  forecaster = AutoARIMA(\n",
        "    start_p = 0, d = 0, start_q = 0, \n",
        "    max_p = max_p, max_d = max_d, max_q = max_q, \n",
        "    sp = sp,\n",
        "    suppress_warnings = True\n",
        "  )\n",
        "  # Una semana dos horas antes es 2016 + 24 = 2040\n",
        "  forecaster.fit(x)\n",
        "\n",
        "  y_pred = forecaster.predict(fh=list(range(1, to_predict + 1)))\n",
        "  return(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsforecast import StatsForecast\n",
        "from statsforecast.models import AutoARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw251Mb_VKqs"
      },
      "outputs": [],
      "source": [
        "def HoltWinters(data,vsele,option='Additive'):\n",
        "  ##'Additive' 'Multiplicative' 'Additive Dam' 'Multiplicative Dam'\n",
        "##https://www.statsmodels.org/dev/examples/notebooks/generated/exponential_smoothing.html#Holt%E2%80%99s-Winters-Seasonal\n",
        "  if option == 'Additive':\n",
        "    fit = ExponentialSmoothing(\n",
        "    data,\n",
        "    seasonal_periods      = vsele,\n",
        "    trend                 = \"add\",\n",
        "    seasonal              = \"add\",\n",
        "    use_boxcox            = True,\n",
        "    initialization_method =\"estimated\").fit()\n",
        "  elif option == 'Multiplicative':\n",
        "    fit = ExponentialSmoothing(\n",
        "    data,\n",
        "    seasonal_periods      = vsele,\n",
        "    trend                 = \"add\",\n",
        "    seasonal              = \"mul\",\n",
        "    use_boxcox            = True,\n",
        "    initialization_method = \"estimated\").fit()\n",
        "  elif option == 'Additive Dam':\n",
        "    fit = ExponentialSmoothing(\n",
        "    data,\n",
        "    seasonal_periods      = vsele,\n",
        "    trend                 = \"add\",\n",
        "    seasonal              = \"add\",\n",
        "    damped_trend          = True,\n",
        "    use_boxcox            = True,\n",
        "    initialization_method = \"estimated\").fit()\n",
        "  elif option == 'Multiplicative Dam':\n",
        "    fit = ExponentialSmoothing(\n",
        "    data,\n",
        "    seasonal_periods      = vsele,\n",
        "    trend                 = \"add\",\n",
        "    seasonal              = \"mul\",\n",
        "    damped_trend          = True,\n",
        "    use_boxcox            = True,\n",
        "    initialization_method = \"estimated\").fit()\n",
        "  return fit.forecast(vsele)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDCxtBbC8wtr"
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa_qq8mqZdG_"
      },
      "outputs": [],
      "source": [
        "## Function that given a datetime, returns its position in the series.\n",
        "def get_index_day(df,day,month,year):\n",
        "    df_i = df.reset_index()  ## Integer indices are added.\n",
        "    idxo = df_i[(df_i.Date.dt.day == day) & (df_i.Date.dt.month==month) & (df_i.Date.dt.year==year)].iloc[ 0]\n",
        "    idxf = df_i[(df_i.Date.dt.day == day) & (df_i.Date.dt.month==month) & (df_i.Date.dt.year==year)].iloc[-1]\n",
        "    return(idxo.name, idxf.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jmt7XJ38Onr6"
      },
      "outputs": [],
      "source": [
        "def get_day_from_index(df, indices, forecast):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame\n",
        "        Original dataframe of the timeseries\n",
        "    indices : array\n",
        "        Array of indices\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    DataFrame\n",
        "        A DataFrame of dates\n",
        "    \"\"\"\n",
        "    df_i = df.reset_index()  ## Integer indices are added.\n",
        "    sel_date = df_i.iloc[indices].Date\n",
        "    t = pd.concat([sel_date.reset_index(), pd.Series(forecast)], axis=1)\n",
        "    t = t.rename(columns={0 : 'value'})\n",
        "    return(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzBO6YghZoin"
      },
      "source": [
        "# Prepare cross-validation for testing\n",
        "\n",
        "We will use the methodology proposed by Rob J Hyndman [cross-validation in time series](https://robjhyndman.com/hyndsight/tscv/#:~:text=Time%20series%20cross%2Dvalidation,used%20in%20constructing%20the%20forecast.\n",
        "), that is a training dataset of four months followed by a testing dataset of the next month.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLECRwaNNpt0",
        "outputId": "5fa7021a-6f55-4c62-bee4-b5c5058ddc0d"
      },
      "outputs": [],
      "source": [
        "## https://robjhyndman.com/hyndsight/tscv/#:~:text=Time%20series%20cross%2Dvalidation,used%20in%20constructing%20the%20forecast.\n",
        "## ... year 2010                                                                                                    year 2011\n",
        "## ... january···february···march···april···may···june···july···august···september···october···november···december···january···february···march···april···may···june···july···august···september···october···november\n",
        "##    |                                   |      |\n",
        "##    |<------------- train ------------->|<test>|\n",
        "##    |<-january·february···march···april->|<may>|\n",
        "##            |<-february···march···april···may->|<june>|\n",
        "##                      |<- march···april···may···june ->|<-july->|\n",
        "##                              |<- april···may···june···july->|<-august->|\n",
        "##                                     |<-- may···june···july···august->|<-september->|\n",
        "##                                             |<-june···july···august···september->|<-october->|\n",
        "##                                                    |<-july···august···september···october->|<-november->|\n",
        "##                                                           |<-august···september···october···november->|<-december->| ...\n",
        "\n",
        "df_5_month = df_5.copy()\n",
        "df_5_month.reset_index(inplace=True)\n",
        "# https://stackoverflow.com/a/25149272\n",
        "df_5_month['month'] = df_5_month['Date'].dt.month\n",
        "df_5_month['year' ] = df_5_month['Date'].dt.year\n",
        "df_5_month = df_5_month.drop_duplicates(['month', 'year'])\n",
        "df_5_month.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJt75hF6PILw"
      },
      "outputs": [],
      "source": [
        "## Get the positions\n",
        "#  to : training initial position\n",
        "#  tt : testing initial position\n",
        "#  tf : time series final position\n",
        "# month_i = df_5_month.index\n",
        "# r = range(len(month_i) - 5)\n",
        "# tuplas = []\n",
        "# for i in r:\n",
        "#   tuplas.append((\n",
        "#     # 1 ene       30 abril            31 may\n",
        "#     month_i[i], month_i[i + 4] - 1, month_i[i + 5] - 1\n",
        "#   ))\n",
        "# tuplas.append((157248, 192671, 200638))\n",
        "# tuplas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRCcdDX3Zbzc"
      },
      "source": [
        "## Public holidays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpS7uXX2Y6ZZ",
        "outputId": "ef45718b-17a5-4c84-f382-d97541ff5528"
      },
      "outputs": [],
      "source": [
        "serie='SERIE1'\n",
        "print(get_index_day(df_5[serie],10,5,2010))   ## mothers day 2010\n",
        "print(get_index_day(df_5[serie],10,5,2011))   ## mothers day 2011\n",
        "print(get_index_day(df_5[serie],24,12,2010))  ## christmas 2010\n",
        "print(get_index_day(df_5[serie],31,12,2010))  ## new year 2011\n",
        "print(get_index_day(df_5[serie],1,1,2011))    ## new year 2011"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-uqPyHPnFof"
      },
      "source": [
        "# Parameter and series selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "iOJ9pmphcnqV",
        "outputId": "2d2cac27-e900-4e1d-a5fa-b3913ccf58d1"
      },
      "outputs": [],
      "source": [
        "serie4 = df_5.SERIE4\n",
        "plt.figure(figsize=(20, 4))\n",
        "#plt.savefig('series1.pdf', bbox_inches = 'tight')\n",
        "serie4.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yvKdtUN6kNd7",
        "outputId": "e8297533-59b0-4ac3-f278-06b9579e36ef"
      },
      "outputs": [],
      "source": [
        "df_serie4 = serie4.reset_index()\n",
        "df_serie4.columns = ['date', 'value']\n",
        "df_serie4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr1AeOKRFgwo"
      },
      "outputs": [],
      "source": [
        "## ene2010... time serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·time-serie·nov2011\n",
        "##                |                                               |       |           |\n",
        "##                |                                               |<-n_p->|           |\n",
        "##                |<------------------ train -------------------->|<-------test------>|\n",
        "##                to                                              tt   ...j=0...      tf \n",
        "\n",
        "n_p       = 30         ## Number of periods per step (Two and half hour)\n",
        "vsele     = 288        ## Number of periods in a window (a day)\n",
        "namefile  = 'forecast'\n",
        "monthyear = 'jun2010'\n",
        "\n",
        "##  21 Junio 2010       - 21 Septiembre 2010   Verano\n",
        "##  21 Septiembre 2010  - 21 Diciembre 2010    Otoño\n",
        "##  21 Diciembre 2011   - 21 Marzo 2011        Invierno\n",
        "##  21 Marzo 2011       - 21 Junio 2011        Primavera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWBOYubUvbLz"
      },
      "outputs": [],
      "source": [
        "df_serie4_summer = df_serie4[(df_serie4.date >='2010-06-21')&(df_serie4.date <'2010-09-21')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xfmCZ6sbPBE"
      },
      "outputs": [],
      "source": [
        "# Each season\n",
        "df_serie4_summer = df_serie4[(df_serie4.date >='2010-06-21')&(df_serie4.date <'2010-09-21')]\n",
        "df_serie4_autum  = df_serie4[(df_serie4.date >='2010-09-21')&(df_serie4.date <'2010-12-21')]\n",
        "df_serie4_winter = df_serie4[(df_serie4.date >='2010-12-21')&(df_serie4.date <'2011-03-21')]\n",
        "df_serie4_spring = df_serie4[(df_serie4.date >='2011-03-21')&(df_serie4.date <'2011-06-21')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSM3A8EgtIbK"
      },
      "source": [
        "## Unit test per season"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "fODm_4FKozqF",
        "outputId": "243ba57a-5b88-4cd0-e8ce-e3c156f43309"
      },
      "outputs": [],
      "source": [
        "df_serie4_summer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "mqDoLp1qc2Yb",
        "outputId": "e3eaf417-d6d4-4422-c61a-86769de46fa1"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(df_serie4_summer.date, df_serie4_summer.value, label = 'Summer')\n",
        "plt.plot(df_serie4_autum.date , df_serie4_autum.value , label = 'Autum')\n",
        "plt.plot(df_serie4_winter.date, df_serie4_winter.value, label = 'Winter')\n",
        "plt.plot(df_serie4_spring.date, df_serie4_spring.value, label = 'Spring')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.legend()\n",
        "#plt.savefig('series2.pdf', bbox_inches = 'tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "M3SjpB_8iPuO",
        "outputId": "8d268852-cf60-4706-8e2d-93c7179548fc"
      },
      "outputs": [],
      "source": [
        "# Data selection for season; i.e. summer\n",
        "indices = df_serie4_summer.date - pd.DateOffset(months=4) # select indices\n",
        "df_serie4_summer_data = df_serie4[\n",
        "  (df_serie4.date >= indices.values[0]) & (df_serie4.index < df_serie4_summer.index[0])\n",
        "]\n",
        "df_serie4_summer_data # train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq565QxQrdD8"
      },
      "outputs": [],
      "source": [
        "t0 = df_serie4_summer_data.iloc[0]  # initial time\n",
        "tt = df_serie4_summer_data.iloc[-1] # time to\n",
        "tf = df_serie4_summer.iloc[-1]      # final time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "QQlHwCUMoFQE",
        "outputId": "f1778fbc-09bb-4f3d-f464-4d4610407458"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(df_serie4_summer_data.date, df_serie4_summer_data.value, label = 'Train')\n",
        "plt.plot(df_serie4_summer.date, df_serie4_summer.value, label = 'Test')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.axvline(x = t0.date, c = 'red')\n",
        "plt.axvline(x = tt.date, c = 'red')\n",
        "plt.axvline(x = tf.date, c = 'red')\n",
        "plt.legend()\n",
        "plt.savefig('fig:traintest.pdf', bbox_inches = 'tight')\n",
        "\n",
        "ax.set_ylabel('Load demand MW')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GecbdIt5se8W",
        "outputId": "ddb3fac2-221b-4186-d458-ed5a6f4b1dfa"
      },
      "outputs": [],
      "source": [
        "# Access index of t0, etc ╰（‵□′）╯\n",
        "t0.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2Wj7EVItTXt"
      },
      "outputs": [],
      "source": [
        "def train_test_season(data, d0 = '2010-06-21', df = '2010-09-21', do = 4, verbose = False):\n",
        "  # d0: start date\n",
        "  # df: final date\n",
        "  # d0: offset date in months\n",
        "  # df_r: df result\n",
        "  # t0: start train index\n",
        "  # tt: end train index / start test index\n",
        "  # tf: end test index\n",
        "  df_r = data[(data.date >= d0) & (data.date < df)]\n",
        "  if verbose:\n",
        "    plt.figure()\n",
        "    plt.plot(df_r.date, df_r.value, label = 'df_r')\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  # Data selection for range(d0, df)\n",
        "  indices = df_r.date - pd.DateOffset(months = do) # select indices\n",
        "  df_r_data = df_serie4[\n",
        "    (df_serie4.date >= indices.values[0]) & (df_serie4.index < df_r.index[0])\n",
        "  ]\n",
        "  t0 = df_r_data.iloc[0]  # initial time\n",
        "  tt = df_r_data.iloc[-1] # time to\n",
        "  tf = df_r.iloc[-1]      # final time\n",
        "\n",
        "  if verbose:\n",
        "    plt.figure()\n",
        "    plt.plot(df_r_data.date, df_r_data.value, label = 'Train')\n",
        "    plt.plot(df_r.date, df_r.value, label = 'Test')\n",
        "    plt.xticks(rotation = 90)\n",
        "    plt.axvline(x = t0.date, c = 'red')\n",
        "    plt.axvline(x = tt.date, c = 'red')\n",
        "    plt.axvline(x = tf.date, c = 'red')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  \n",
        "  return (t0, tt, tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o3of7MNvvd6",
        "outputId": "e0646c92-eca7-4e24-ed86-5d3dd8865b8b"
      },
      "outputs": [],
      "source": [
        "t0, tt, tf = train_test_season(df_serie4, d0 = '2010-06-21', df = '2010-09-21', do = 4, verbose = False)\n",
        "#print(t0, tt, tf)\n",
        "print(t0.name, tt.name, tf.name) # indices\n",
        "t0=t0.name\n",
        "tt=tt.name\n",
        "tf=tf.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU0Q58g8Eb-k"
      },
      "source": [
        "# Multi-period forecast metafunction **II**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA8gpzWitiii"
      },
      "source": [
        "## Test series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v14OmdADCpQa",
        "outputId": "04047a22-68f3-468f-dcd8-0029a72faf7d"
      },
      "outputs": [],
      "source": [
        "serie1 = serie4.to_numpy(serie4)\n",
        "serie1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0t1NvnhFgwo"
      },
      "outputs": [],
      "source": [
        "## Prepare the test series (real data)\n",
        "\n",
        "positions_test = [(t0, tt, tf)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "3YlOMb4TaFdL",
        "outputId": "8d4acc46-a7da-4c50-faf1-15d50c8b4f71"
      },
      "outputs": [],
      "source": [
        "## Non-numeric\n",
        "nonumeric=[]\n",
        "j=0\n",
        "aux=100\n",
        "for item in serie1:\n",
        "  if math.isnan(item):\n",
        "    nonumeric.append(j)   \n",
        "    print(j,df_5[serie].index[j],df_5[serie][j])\n",
        "  j=j+1\n",
        "#plt.plot(nonumeric)\n",
        "#plt.plot(serie1[131640 -aux:131651 +aux])\n",
        "#print(df_5[serie].index[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzofrT84fO2v"
      },
      "outputs": [],
      "source": [
        "for to,tt,tf in positions_test:\n",
        "  j=0\n",
        "  s=n_p\n",
        "  for i in range(tt,tf,1):\n",
        "    X_train = numpy.array(serie1[to+j:tt+j])\n",
        "    pred_, t_sel_, t_reg_, fail_, positions = analogo_knn(X_train, vsele=288, k = 10, tol = 0.8, typedist = 'pearson', typereg = 'PCR', verbose=False)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "positions_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9s5aRTMOfoqq"
      },
      "outputs": [],
      "source": [
        "n = len(X_train)\n",
        "Y = X_train[n - 288 : n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnAWmpEtfwRf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.legend(['First line', 'Second line'])\n",
        "plt.plot([], [], label='$X_k$', c = 'cornflowerblue', alpha = 0.4)\n",
        "plt.plot([], [], '--', label='$X\\'_k$', c = 'orange', alpha = 0.4)\n",
        "for i in range(len(positions)):\n",
        "  serie = X_train[positions[i] : positions[i]+vsele]\n",
        "  serie2 = X_train[positions[i] + vsele : positions[i] + 2*vsele]\n",
        "  plt.plot(serie, c = 'cornflowerblue', alpha = 0.4)\n",
        "  plt.plot(range(vsele, vsele*2), serie2, '--', c = 'orange', alpha = 0.4)\n",
        "\n",
        "plt.plot(Y,label='Y', color='cornflowerblue', linewidth = 3)   \n",
        "plt.plot(range(vsele, vsele*2), pred_, label='$Y\\'$', color='mediumseagreen',linestyle='--', linewidth = 3)      \n",
        "\n",
        "plt.axvline(x = vsele, linestyle='--', c = 'k', linewidth = 1)\n",
        "\n",
        "plt.xlabel('Time (5 min)', fontsize = 16)\n",
        "plt.ylabel('Demand (MW)', fontsize = 16)\n",
        "plt.xticks(fontsize = 16)\n",
        "plt.yticks(fontsize = 16)\n",
        "\n",
        "plt.legend(bbox_to_anchor=(1, 0.7), fontsize = 16)\n",
        "plt.tight_layout()\n",
        "#plt.savefig('neighbors_example.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AR-MA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_V4YDtfFgwp"
      },
      "outputs": [],
      "source": [
        "timeAn_         = []\n",
        "forecastAn_     = []\n",
        "forecastAnMA_   = []\n",
        "forecastX_An_   = []\n",
        "forecastX_AnMA_ = []\n",
        "row             = []\n",
        "rowX            = []\n",
        "row2            = []\n",
        "rowX2           = []\n",
        "k               = 5          ## Number of nearest neighbors\n",
        "tol             = 0.8        ## Closest tolerance percentage between neighbors\n",
        "typedist        = ''  ## Distance betweeen neighbors: 'pearson' 'euclidian' 'lb_keogh' 'matrixprofile'\n",
        "typereg         = ''  ## Regression model: 'OLSstep' 'Boosting' 'Bagging' 'LinearReg' 'AdaBoost' 'BayesRidge' 'LassoReg' 'RidgeReg' 'PLS' 'PCR' 'VotingEnsemble' 'VotingLinear'\n",
        "n_p             = n_p        ## Number of periods per step\n",
        "nfail           = 0\n",
        "fail_           = False\n",
        "vsele           = vsele      ## Number of periods in a window\n",
        "for to,tt,tf in positions_test:\n",
        "    j=0\n",
        "    s=n_p\n",
        "    for i in range(tt,tf,1):\n",
        "        ## Analogue method parameters\n",
        "        X_train = numpy.array(serie1[to+j:tt+j])\n",
        "        t_o     = time.time()\n",
        "        try:\n",
        "            ar = AutoReg(X_train, lags=288*7).fit()         \n",
        "            pred_ = ar.forecast(n_p)\n",
        "        except:\n",
        "            print(\"!!! Error has occurred in the position:\",tt+j)\n",
        "            # row = ['!!! Error has occurred in the position:',tt+j]\n",
        "            # append_list_as_row('LogAn.csv',row)\n",
        "        if fail_==True:              \n",
        "            nfail = nfail + 1\n",
        "            print(\">>> Persistence forecast in position:\",tt+j)\n",
        "        pred_list  = pred_.tolist()\n",
        "        forecastAn_ = forecastAn_ + pred_list[0:1]\n",
        "        row = [tt+j] + [time.time() - t_o] + pred_list[0:1]\n",
        "        append_list_as_row('AR.csv',row)\n",
        "        print(j+tt)\n",
        "        # Minimum error in the first forecast\n",
        "        if j >= 3 * n_p:\n",
        "            a = - 1 - min(j,vsele) \n",
        "            b = - 1\n",
        "            array1 = np.array(forecastAn_[a:b])\n",
        "            c = tt + j - min(j,vsele)\n",
        "            d = tt + j\n",
        "            array2 = np.array(serie1[c:d])                         \n",
        "            epsilon = np.subtract(array1,array2)  ## Errores del primer pronóstico\n",
        "            ar = AutoReg(epsilon, lags=int(n_p/2)).fit() ## Ajuste de los últimos n_p/2          \n",
        "            delta = ar.forecast(1)\n",
        "            MA_ = pred_list[0:1] - delta\n",
        "            forecastAnMA_.extend(MA_)\n",
        "            MA_ = [MA_]\n",
        "            row = [tt+j] + [time.time() - t_o] + MA_ \n",
        "            append_list_as_row('ARMA.csv',row)\n",
        "            if s==n_p:\n",
        "                forecastX_An_   = forecastX_An_ + pred_list[0:n_p]\n",
        "                for val in pred_list[0:n_p]:\n",
        "                    row = [tt+j] + [time.time() - t_o] + [val]\n",
        "                    append_list_as_row('XAR.csv',row)\n",
        "\n",
        "                MA_ = pred_list[0:n_p] - delta\n",
        "                forecastX_AnMA_.extend(MA_)\n",
        "                for val in MA_[0:n_p]:\n",
        "                    row = [tt+j] + [time.time() - t_o] + [val]\n",
        "                    append_list_as_row('XARMA.csv',row)\n",
        "                s=0\n",
        "        else:\n",
        "            if s==n_p:\n",
        "                forecastX_An_   = forecastX_An_ + pred_list[0:n_p]\n",
        "                forecastX_AnMA_ = forecastX_An_\n",
        "                for val in pred_list[0:n_p]:\n",
        "                    row = [tt+j] + [time.time() - t_o] + [val]\n",
        "                    append_list_as_row('XAR.csv',row)\n",
        "                    append_list_as_row('XARMA.csv',row)\n",
        "                s=0\n",
        "            forecastAnMA_ = forecastAn_\n",
        "            row = [tt+j] + [time.time() - t_o] + [forecastAn_[-1]] \n",
        "            append_list_as_row('ARMA.csv',row)\n",
        "\n",
        "        timeAn_.append(time.time() - t_o)\n",
        "        j = j + 1\n",
        "        s = s + 1\n",
        "\n",
        "    modu=(tf-tt)%n_p\n",
        "    if modu != 0:\n",
        "        end=min(len(forecastAn_),len(forecastX_An_))\n",
        "        print('modu',modu)\n",
        "        forecastAn_     = forecastAn_[    0:end]\n",
        "        forecastAnMA_   = forecastAnMA_[  0:end]\n",
        "        forecastX_AnMA_ = forecastX_AnMA_[0:end]        \n",
        "        forecastX_An_   = forecastX_An_[  0:end]\n",
        "print('>>> Number of forecasts not calculated:', nfail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14IgL8qeFgwq"
      },
      "outputs": [],
      "source": [
        "date_i = df_5.reset_index().iloc[tt].Date\n",
        "print('analog:',typereg,typedist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# #PROPUESTA ALTERNATIVA DE SUMAR TODOS LOS MA al epsilon\n",
        "# n_p=5\n",
        "# pred_list= [0,0,0,0,0]\n",
        "# epsilon=[2,3,4,5,2,3,4,5,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,]\n",
        "# epsilon=np.array(epsilon) \n",
        "# ar = AutoReg(epsilon, lags=int(n_p/2)).fit() ## Ajuste de los últimos n_p/2          \n",
        "# print(epsilon)\n",
        "# delta = ar.forecast(5)\n",
        "# print(delta)\n",
        "# MA_ = pred_list[0:1] - delta\n",
        "# MA_ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzEi2sWkFgwq"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame({\n",
        "  'datetime'          : pd.date_range(date_i, periods=len(test_), freq=\"5T\"),\n",
        "  'Actual'            : test_,\n",
        "  # 'Persistence'       : forecastp_,\n",
        "  # 'An '    +typedist+'+'+typereg: forecastAn_,\n",
        "  # 'AnMA '  +typedist+'+'+typereg: forecastAnMA_,\n",
        "  # 'HWA '                         : forecastHWA_,\n",
        "  # 'HWM '                         : forecastHWM_,\n",
        "  # 'ARIMA '                       : forecastARIMA_,\n",
        "  'X_An '  +typedist+'+'+typereg: forecastX_An_,\n",
        "  'X_AnMA '+typedist+'+'+typereg: forecastX_AnMA_,\n",
        "  'X_HWA '                       : forecastX_HWA_,\n",
        "  'X_HWM '                       : forecastX_HWM_,\n",
        "  'X_ARIMA '                     : forecastX_ARIMA_,\n",
        "})\n",
        "df_results = df_results.set_index('datetime')\n",
        "\n",
        "## Save in csv files\n",
        "df_results.to_csv(namefile+'_'+serie+'_'+monthyear+'.csv')\n",
        "\n",
        "plt.figure(figsize=(40, 10))\n",
        "for c in df_results:\n",
        "  if c == 'Actual':\n",
        "    plt.plot(df_results[c], '--', label = c)\n",
        "  else:\n",
        "    plt.plot(df_results[c], label = c, alpha=0.8)\n",
        "plt.legend()\n",
        "plt.ylabel('demand (MW)')\n",
        "plt.savefig('series_'+monthyear+'.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5qesanVPNW3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_86i3uHFgwq"
      },
      "outputs": [],
      "source": [
        "a=0; b=2000\n",
        "mae_p  = mean_absolute_error(test_[a:b],forecastp_[a:b])\n",
        "mape_p = mean_absolute_percentage_error(test_[a:b],forecastp_[a:b])\n",
        "bias_p = bias.bias(test_[a:b],forecastp_[a:b])\n",
        "\n",
        "mae_a  = mean_absolute_error(test_[a:b],forecastAn_)\n",
        "mape_a = mean_absolute_percentage_error(test_[a:b],forecastAn_[a:b])\n",
        "bias_a = bias.bias(test_[a:b],forecastAn_[a:b])\n",
        "\n",
        "mae_ma  = mean_absolute_error(test_[a:b],forecastAnMA_[a:b])\n",
        "mape_ma = mean_absolute_percentage_error(test_[a:b],forecastAnMA_[a:b])\n",
        "bias_ma = bias.bias(test_[a:b],forecastAnMA_[a:b])\n",
        "\n",
        "mae_xa  = mean_absolute_error(test_[a:b],forecastX_An_)\n",
        "mape_xa = mean_absolute_percentage_error(test_[a:b],forecastX_An_[a:b])\n",
        "bias_xa = bias.bias(test_[a:b],forecastX_An_[a:b])\n",
        "\n",
        "mae_xma  = mean_absolute_error(test_[a:b],forecastX_AnMA_)\n",
        "mape_xma = mean_absolute_percentage_error(test_[a:b],forecastX_AnMA_[a:b])\n",
        "bias_xma = bias.bias(test_[a:b],forecastX_AnMA_[a:b])\n",
        "\n",
        "mae_hwa  = mean_absolute_error(test_[a:b],forecastHWA_)\n",
        "mape_hwa = mean_absolute_percentage_error(test_[a:b],forecastHWA_[a:b])\n",
        "bias_hwa = bias.bias(test_[a:b],forecastHWA_[a:b])\n",
        "\n",
        "mae_xhwa  = mean_absolute_error(test_[a:b],forecastX_HWA_)\n",
        "mape_xhwa = mean_absolute_percentage_error(test_[a:b],forecastX_HWA_[a:b])\n",
        "bias_xhwa = bias.bias(test_[a:b],forecastX_HWA_[a:b])\n",
        "\n",
        "mae_hwm  = mean_absolute_error(test_[a:b],forecastHWM_)\n",
        "mape_hwm = mean_absolute_percentage_error(test_[a:b],forecastHWM_[a:b])\n",
        "bias_hwm = bias.bias(test_[a:b],forecastHWM_[a:b])\n",
        "\n",
        "mae_xhwm  = mean_absolute_error(test_[a:b],forecastX_HWM_)\n",
        "mape_xhwm = mean_absolute_percentage_error(test_[a:b],forecastX_HWM_[a:b])\n",
        "bias_xhwm = bias.bias(test_[a:b],forecastX_HWM_[a:b])\n",
        "\n",
        "mae_arima  = mean_absolute_error(test_[a:b],forecastX_ARIMA_)\n",
        "mape_arima  = mean_absolute_percentage_error(test_[a:b],forecastX_ARIMA_[a:b])\n",
        "bias_arima  = bias.bias(test_[a:b],forecastX_ARIMA_[a:b])\n",
        "\n",
        "mae_xarima  = mean_absolute_error(test_[a:b],forecastX_ARIMA_)\n",
        "mape_xarima = mean_absolute_percentage_error(test_[a:b],forecastX_ARIMA_[a:b])\n",
        "bias_xarima = bias.bias(test_[a:b],forecastX_ARIMA_[a:b])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFD4_sbIFgwr"
      },
      "outputs": [],
      "source": [
        "## One-period\n",
        "d      = typedist\n",
        "m      = typereg\n",
        "data   = [diff(test_,forecastp_), diff(test_,forecastAn_), diff(test_,forecastAnMA_),\n",
        "          diff(test_,forecastHWA_), diff(test_,forecastHWM_),\n",
        "          ]\n",
        "labels = ['Persistence', 'An\\n('+d+'+'+m+')', 'AnMA\\n('+d+'+'+m+')',\n",
        "                         'HWA',  'HWM',]\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
        "CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; LETRASNARA ='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"#FF0000\"; # 'pink', 'lightblue', 'lightgreen',\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "bplot1 = axes.boxplot(data,\n",
        "                      vert=True,          # vertical box alignment\n",
        "                      patch_artist=True,  # fill with color\n",
        "                      labels=labels,)     # will be used to label x-ticks\n",
        "#axes.set_title('Accuracy',fontsize='x-large',color = LETRASNARA)\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "colors = ['pink', 'lightblue', 'lightgreen',LILA, AZUL] # fill with colors\n",
        "for patch, color in zip(bplot1['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "axes.yaxis.grid(True) # adding horizontal grid lines\n",
        "#axes.set_xlabel('Configuraciones de red',fontsize='large',color = LETRASNARA)\n",
        "axes.set_ylabel('')\n",
        "namefile = 'boxplot_'+monthyear+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCQs-f1UAXbo"
      },
      "outputs": [],
      "source": [
        "## Multi-period\n",
        "d      = typedist\n",
        "m      = typereg\n",
        "data   = [ diff(test_,forecastX_An_), diff(test_,forecastX_AnMA_), \n",
        "          diff(test_,forecastX_HWA_), diff(test_,forecastX_HWM_),\n",
        "           diff(test_,forecastX_ARIMA_), #diff(test_,forecastARIMA_),\n",
        "          ]\n",
        "labels = [ 'X_An\\n('+d+'+'+m+')', 'X_AnMA\\n('+d+'+'+m+')',\n",
        "                          'X_HWA',  'X_HWM', 'X_ARIMA',]\n",
        "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\n",
        "CYAN = '#76ced6' ; LILA = '#777bd4'; VERDE='#17cb49'; LETRASNARA ='#ff8000'; AZUL='#168fff'; OTROAZUL = \"b-\"; ROJO= \"#FF0000\"; # 'pink', 'lightblue', 'lightgreen',\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "bplot1 = axes.boxplot(data,\n",
        "                      vert=True,          # vertical box alignment\n",
        "                      patch_artist=True,  # fill with color\n",
        "                      labels=labels,)     # will be used to label x-ticks\n",
        "#axes.set_title('Accuracy',fontsize='x-large',color = LETRASNARA)\n",
        "#plt.tick_params(colors = LETRASNARA, which='both')\n",
        "colors = ['pink', 'lightblue', 'lightgreen',LILA, AZUL] # fill with colors\n",
        "for patch, color in zip(bplot1['boxes'], colors):\n",
        "    patch.set_facecolor(color)\n",
        "axes.yaxis.grid(True) # adding horizontal grid lines\n",
        "#axes.set_xlabel('Configuraciones de red',fontsize='large',color = LETRASNARA)\n",
        "axes.set_ylabel('')\n",
        "namefile = 'boxplot_'+monthyear+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzczX8evFgwr"
      },
      "outputs": [],
      "source": [
        "plt.plot(timeAn_, label=\"AnMA\")\n",
        "plt.plot(timeHWA_, label=\"HWA\")\n",
        "plt.plot(timeHWM_, label=\"HWM\")\n",
        "plt.plot(timeARIMA_, label=\"ARIMA\")\n",
        "plt.ylabel('compute seconds')\n",
        "plt.xlabel('time')\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "print('time_'+monthyear+'.pdf',((np.mean(timeAn_)*8723)/60)/60 )\n",
        "\n",
        "namefile = 'timeAn_'+monthyear+'.pdf'\n",
        "plt.savefig(namefile, transparent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#PROPUESTA ALTERNATIVA DE SUMAR TODOS LOS MA al epsilon\n",
        "n_p=5\n",
        "pred_list= [0,0,0,0,0]\n",
        "epsilon=[2,3,4,5,2,3,4,5,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,2,3,4,5,2,3,4,]\n",
        "epsilon=np.array(epsilon) \n",
        "ar = AutoReg(epsilon, lags=int(n_p/2)).fit() ## Ajuste de los últimos n_p/2          \n",
        "print(epsilon)\n",
        "delta = ar.forecast(5)\n",
        "print(delta)\n",
        "MA_ = pred_list[0:1] - delta\n",
        "MA_ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7j41VGHG6eO"
      },
      "source": [
        "# References\n",
        "\n",
        "*   Jason Brownlee. Deep Learning for Time Series Forecasting: Predict the Future with MLPs, CNNs and LSTMs in Python, (book) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4IaaVa4OnsM"
      },
      "source": [
        "# To-do\n",
        "\n",
        "* [ ] Prueba estadística para ver diferencia significativa entre An y AnMA\n",
        "* [ ] Ajustar el ARIMA\n",
        "* [ ] Dividir el radial de BIAS y los MAPE en los multi periodo y uni período (pasar a bar plots)\n",
        "* [ ] Almacenar los resultados en csv\n",
        "* [ ] Violin plots de error\n",
        "* [ ] LSTM"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Analog.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
