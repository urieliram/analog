{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aplicación de RNN y CV.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urieliram/analog/blob/main/Aplicaci%C3%B3n_de_RNN_y_CV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVk2XybB_Av6"
      },
      "source": [
        "A pesar de los esfuerzos de los gobiernos por implementar medidas estrictas de mitigación y/o supresión, el COVID-19 ha tenido un impacto profundo en los perfiles de demanda eléctrica en todo el mundo. Entre marzo, abril y mayo de 2020, la demanda eléctrica mundial se desplomó en todo el planeta, y países como España e Italia experimentaron una disminución de más del 20% en su consumo eléctrico habitual. Como consecuencia de estos cambios masivos en la demanda eléctrica, y a pesar del esfuerzo de los  sistemas de pronóstico por proporcionar una predicción precisa de la demanda, los operadores de las redes eléctricas se enfrentan a retos importantes para programar los recursos energéticos e infrestructura necesaria. \n",
        "\n",
        "En este ejercicio, haremos un pronóstico multivariado a múltiples pasos utilizando una red recurrente LSTM apilada. Se utilizará transfer learning para reusar el modelo construído con datos anteriores a la pandemia para obtener un modelo para el período de pandemia.\n",
        "\n",
        "Los datos incluyen demanda mensual, observaciones meteorológicas y pronósticos  meteorológicos. Estos datos fueron obtenidos de proveedores de servicios meteorológicos y de servicios públicos reales y, por lo tanto, pueden estar contaminados con períodos perdidos, anomalías, etc. La fuente de la información es confidencial pero fue utilizada para el [concurso de pronóstico de demanda eléctrica al día siguiente de la IEEE](https://dx.doi.org/10.21227/67vy-bs34.s ).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2gEf2PQ1LzT"
      },
      "source": [
        "# Pronóstico multivariado a múltiples pasos con redes LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRh3E6iPHgaV"
      },
      "source": [
        "# Redes neuronales\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Utilidades numéricas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Graficación\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Utilidades - aprendizaje automático\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Utilidades - barras de carga\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CncY5fXQzQW"
      },
      "source": [
        "## Cargar datos, Preprocesamiento e Ingeniería de características"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YziusvRVy7ci",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "12222592-e46a-431a-f0a4-840e0fe46fb1"
      },
      "source": [
        "# montamos la unidad de drive en colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_path='gdrive/My Drive/INEEL/cemie redes/PE-A-09/datasets/Actuals.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-032fd2282a6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# montamos la unidad de drive en colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gdrive/My Drive/INEEL/cemie redes/PE-A-09/datasets/Actuals.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 124\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE7_rr6ZJWcJ"
      },
      "source": [
        "# Leer datos\n",
        "df = pd.read_csv(data_path, decimal=',')\n",
        "\n",
        "# Convertir a tipo de dato de fecha\n",
        "df['Time'] = pd.to_datetime(df['Time'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Convertir a valores numéricos\n",
        "dtypes = {col:'float' for col in df.columns[1:]}\n",
        "df = df.astype(dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXRpOlnxyp8N"
      },
      "source": [
        "Extraer información de día de la semana, mes y hora a partir de la fecha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEmwwuL6JaJ4"
      },
      "source": [
        "df['day_of_week'] = df.Time.dt.dayofweek\n",
        "df['month'] = df.Time.dt.month\n",
        "df['hour'] = df.Time.dt.hour"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltFAoVjpy1wv"
      },
      "source": [
        "Codificar la ciclicidad de la dirección del viento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVm3Jg6HOnvX"
      },
      "source": [
        "df['wind_dir_sin'] = np.sin(df['Wind Direction (deg)'] * np.pi / 180.)\n",
        "df['wind_dir_cos'] = np.cos(df['Wind Direction (deg)'] * np.pi / 180.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OP_QT1Oy6fT"
      },
      "source": [
        "Preprocesamiento general"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgBEaEoyTkAq"
      },
      "source": [
        "# Indexar por fecha\n",
        "df.set_index('Time', inplace=True)\n",
        "\n",
        "# Renombrar columnas\n",
        "rename_cols = {\n",
        "  'Load (kW)': 'load',\n",
        "  'Pressure_kpa': 'pressure',\n",
        "  'Cloud Cover (%)': 'cloud_cover',\n",
        "  'Humidity (%)': 'humidity',\n",
        "  'Temperature (C) ': 'temperature',\n",
        "  'Wind Speed (kmh)': 'wind_speed',\n",
        "}\n",
        "\n",
        "df.rename(columns=rename_cols, inplace=True)\n",
        "\n",
        "cols = [\n",
        "  'load',\n",
        "  'pressure',\n",
        "  'cloud_cover',\n",
        "  'humidity',\n",
        "  'temperature',\n",
        "  'wind_speed',\n",
        "  'day_of_week',\n",
        "  'month',\n",
        "  'hour',\n",
        "  'wind_dir_sin',\n",
        "  'wind_dir_cos'\n",
        "]\n",
        "\n",
        "df = df[cols]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1fhioAKUD4T"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g5MCSZSSokB"
      },
      "source": [
        "## Exploración de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvKnnhO3zHFP"
      },
      "source": [
        "La variable de interés es `load` que corresponde al consumo energético.\n",
        "\n",
        "Graficamos esta variable para observar qué patrones tiene."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx2n9oi_Jehk"
      },
      "source": [
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(data=df, y='load', x='Time')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn59F6zWzS5r"
      },
      "source": [
        "Podemos observar un cambio drástico en el comportamiento de la serie al iniciar la cuarentena de COVID-19.\n",
        "\n",
        "Analizaremos cada segmento de la serie por separado.\n",
        "\n",
        "Entrenaremos un primer modelo predictivo para el comportamiento antes de la cuarentena. Posteriormente, usando transferencia de aprendizaje, partiremos de este primer modelo para hacer uno entrenado para predecir el energético durante la cuarentena."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geiAko10SNyE"
      },
      "source": [
        "# Dividr datos antes y después del inicio de la cuarentena\n",
        "# bc: before covid; ac: after covid\n",
        "data_bc = df.loc[:'2020-03-7']\n",
        "data_ac = df.loc['2020-03-7':]\n",
        "print('Data before COVID-19:', len(data_bc), sep='\\t')\n",
        "print('Data after COVID-19:', len(data_ac), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lui6L99zTJmS"
      },
      "source": [
        "## Predicción de consumo energético previo a la cuarentena\n",
        "\n",
        "### Procesamiento de datos para entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5WrIvfs1a2S"
      },
      "source": [
        "Dividimos los datos en dos conjuntos, entrenamiento y validación"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xB3uYwaNU3I"
      },
      "source": [
        "train_data, test_data = train_test_split(data_bc, train_size=0.8, shuffle=False)\n",
        "print('Training data:', len(train_data), sep='\\t\\t')\n",
        "print('Validation data:', len(test_data), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU1ct6-n1ukb"
      },
      "source": [
        "Normalizamos los datos. Nótese que el normalizador se calcula con los datos de entrenamiento para evitar que se filtre información del conjunto de datos de validación al de entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmJW6T5WU4X-"
      },
      "source": [
        "# Normalizar datos\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Obtener parámetros para normalizar de los datos de entrenamiento\n",
        "train_data_normalized = scaler.fit_transform(train_data)\n",
        "\n",
        "# Normalizar con los mismos parámetros a los datos de validación\n",
        "test_data_normalized = scaler.transform(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgxEqS42ETP"
      },
      "source": [
        "Creamos una clase para cargar los datos al modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrNH8gGfVcF4"
      },
      "source": [
        "class TimeseriesDataset(torch.utils.data.Dataset):\n",
        "  \"\"\" Clase para crear conjuntos de datos para series de tiempo\n",
        "\n",
        "      Dada una serie de tiempo (posiblemente multivariada),\n",
        "      crea conjuntos de datos de entrada y salida según un tamaño de ventana\n",
        "      y un horizonte de predicción\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, X, y, window, horizon):\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "    self.window = window\n",
        "    self.horizon = horizon\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.X.__len__() - self.horizon - self.window\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return (\n",
        "      self.X[index:index + self.window],\n",
        "      self.y[index + self.window: index + self.window + self.horizon]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bgbpi5mAAct"
      },
      "source": [
        "Creamos el cargador de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNqU1xOtVxli"
      },
      "source": [
        "# Valores previos para predicción: 1 semana de valores horarios\n",
        "window = 24 * 7\n",
        "\n",
        "# Horizonte de predicción: 1 día con valores horarios\n",
        "horizon = 24\n",
        "\n",
        "# Ejemplos a considerar por paso de entrenamiento\n",
        "batch_size = 16\n",
        "\n",
        "train_dataset = TimeseriesDataset(train_data_normalized, train_data_normalized[:, 0], window, horizon)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TimeseriesDataset(test_data_normalized, test_data_normalized[:, 0], window, horizon)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAJW2EBgTfXS"
      },
      "source": [
        "## Modelado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbd8KHumAIJG"
      },
      "source": [
        "El modelo es una red neuronal recurrente compuesta por dos capas de unidades LSTM seguidas por una capa densa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEz8unoEThAS"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, n_features, n_hidden, n_layers):\n",
        "    super().__init__()\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Capa LSTM\n",
        "    self.l_lstm = nn.LSTM(input_size=n_features, \n",
        "                          hidden_size=self.n_hidden,\n",
        "                          num_layers=self.n_layers, \n",
        "                          batch_first=True)\n",
        "  \n",
        "    # Capa Densa de activación lineal\n",
        "    self.l_linear = nn.Linear(self.n_hidden * window, horizon)\n",
        "\n",
        "    # Estado escondido\n",
        "    self.hidden_cell = (torch.zeros(1,1,self.n_hidden),) * n_hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
        "    cell_state = torch.zeros(self.n_layers,batch_size,self.n_hidden)\n",
        "    self.hidden = (hidden_state, cell_state)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, seq_len, _ = x.size()\n",
        "    lstm_out, self.hidden = self.l_lstm(x,self.hidden)\n",
        "    x = lstm_out.contiguous().view(batch_size,-1)\n",
        "    return self.l_linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvfyrS-ARn3"
      },
      "source": [
        "Determinamos los hiperparámetros de la red.\n",
        "\n",
        "A mayor número de capas (`n_layers`) la red tendrá capacidad de encontrar patrones de mayor abstracción.\n",
        "\n",
        "A mayor cantidad de celdas ocultas (`n_hidden`) la red tendrá más capacidad para ajustarse a los datos.\n",
        "\n",
        "Los mejores valores para cada hiperparámetro dependerán del problema. Para encontrar los óptimos se puede aplicar optimización de hiperparámetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnWMcs3aj5cN"
      },
      "source": [
        "# Hiperparámetros de la red\n",
        "n_features = len(cols)\n",
        "n_hidden = 200\n",
        "n_layers = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el6Mt_HZBcDU"
      },
      "source": [
        "Instanciamos la red y el optimizador. Utilizaremos el error cuadrático medio (MSE) como función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A08dO7RXA1X"
      },
      "source": [
        "model = LSTM(n_features, n_hidden, n_layers)\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RWcBwKBXFNE"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2r_srCVBvi4"
      },
      "source": [
        "Entrenamos por 5 épocas. Es decir, se ajustará el modelo a los datos de entrenamiento haciendo 5 pasados sobre todo el conjunto.\n",
        "\n",
        "En cada época se imprime el error de entrenamiento y de validación."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvkjGqYuXK4F"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "for i in range(epochs):\n",
        "  print('Epoch:', i + 1)\n",
        "\n",
        "  # Train\n",
        "  print('Train')\n",
        "  model.train(True)\n",
        "  for inpt, target in tqdm(train_loader):\n",
        "    model.init_hidden(inpt.size(0))\n",
        "    output = model(inpt.float()) \n",
        "    loss = loss_function(output, target.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()  \n",
        "    optimizer.zero_grad() \n",
        "  print('Train loss :', loss.item())\n",
        "\n",
        "  # Test\n",
        "  print('\\nTest')\n",
        "  model.train(False)\n",
        "  running_loss = 0\n",
        "  for inpt, target in tqdm(test_loader):\n",
        "    model.init_hidden(inpt.size(0))\n",
        "    output = model(inpt.float()) \n",
        "    running_loss += loss_function(output, target.float()).item()\n",
        "\n",
        "  print('Test loss :', running_loss / len(test_loader))\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQFlfx12J8Zl"
      },
      "source": [
        "model.train(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRcLfZVDirGR"
      },
      "source": [
        "def plot_past_future(X, y):\n",
        "  ex_input = torch.Tensor(X)\n",
        "\n",
        "  unscaled_past = scaler.inverse_transform(ex_input)[:, 0]\n",
        "\n",
        "  ex_input = ex_input.unsqueeze(0)\n",
        "\n",
        "  model.init_hidden(ex_input.size(0))\n",
        "  ex_output = model(ex_input)\n",
        "\n",
        "  ex_output_array = np.zeros((24, 11))\n",
        "  ex_output_array[:, 0] = ex_output.detach().numpy()\n",
        "  unscaled_output = scaler.inverse_transform(ex_output_array)[:, 0]\n",
        "\n",
        "  real_future = np.zeros((24, 11))\n",
        "  real_future[:, 0] = y\n",
        "  unscaled_future = scaler.inverse_transform(real_future)[:, 0]\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "\n",
        "  plt.plot(np.arange(-window, 0), unscaled_past, label='past')\n",
        "\n",
        "  plt.plot(np.arange(horizon), unscaled_future, label='real')\n",
        "  plt.plot(np.arange(horizon), unscaled_output, '.', label='prediction')\n",
        "  plt.legend(loc=\"lower left\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZNJHxUaCCJy"
      },
      "source": [
        "Tomamos una muestra del conjunto de validación para observar cómo se ven las predicciones del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50aHhaVqKCMU"
      },
      "source": [
        "for _ in range(10):\n",
        "  example = test_dataset[np.random.randint(len(test_dataset))]\n",
        "  plot_past_future(*example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAGi2ah4CLa2"
      },
      "source": [
        "Graficamos todas las predicciones en el conjunto de datos de validación para observar el comportamiento general del modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVcuzDdjMjR8"
      },
      "source": [
        "total_mae = 0\n",
        "total_mape = 0\n",
        "\n",
        "n = 500\n",
        "# n = len(test_dataset)\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(test_data.to_numpy()[:n + horizon, 0])\n",
        "\n",
        "for i, (inpt, target) in enumerate(tqdm(test_loader)):\n",
        "  if i > n:\n",
        "    break\n",
        "  unscaled_past = scaler.inverse_transform(inpt[0, :, :])[:, 0]\n",
        "\n",
        "  model.init_hidden(inpt.size(0))\n",
        "  output = model(inpt.float()) \n",
        "  \n",
        "  output_array = np.zeros((horizon, n_features))\n",
        "  output_array[:, 0] = output.detach().numpy()\n",
        "  unscaled_output = scaler.inverse_transform(output_array)[:, 0]\n",
        "\n",
        "  real_future = np.zeros((horizon, n_features))\n",
        "  real_future[:, 0] = target\n",
        "  unscaled_future = scaler.inverse_transform(real_future)[:, 0]\n",
        "\n",
        "  plt.plot(np.arange(i, i + horizon), unscaled_output, alpha=0.04, color='green')\n",
        "\n",
        "  mae = np.absolute(unscaled_output - unscaled_future)\n",
        "  mae = np.sum(mae)\n",
        "  total_mae += mae\n",
        "\n",
        "  mape = np.absolute((unscaled_output - unscaled_future) / unscaled_future)\n",
        "  mape = np.sum(mape)\n",
        "  total_mape += mape\n",
        "\n",
        "plt.show()\n",
        "\n",
        "total_mae /= n * horizon\n",
        "total_mape /= n * horizon\n",
        "\n",
        "print('Validation MAE:', total_mae)\n",
        "print('Validation MAPE:', total_mape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1b3pPE83jBT"
      },
      "source": [
        "total_mae = 0\n",
        "total_mape = 0\n",
        "\n",
        "# n = 500\n",
        "n = len(test_dataset)\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(test_data.to_numpy()[:n + horizon, 0])\n",
        "\n",
        "for i, (inpt, target) in enumerate(tqdm(test_loader)):\n",
        "  if i > n:\n",
        "    break\n",
        "  unscaled_past = scaler.inverse_transform(inpt[0, :, :])[:, 0]\n",
        "\n",
        "  model.init_hidden(inpt.size(0))\n",
        "  output = model(inpt.float()) \n",
        "  \n",
        "  output_array = np.zeros((horizon, n_features))\n",
        "  output_array[:, 0] = output.detach().numpy()\n",
        "  unscaled_output = scaler.inverse_transform(output_array)[:, 0]\n",
        "\n",
        "  real_future = np.zeros((horizon, n_features))\n",
        "  real_future[:, 0] = target\n",
        "  unscaled_future = scaler.inverse_transform(real_future)[:, 0]\n",
        "\n",
        "  plt.plot(np.arange(i, i + horizon), unscaled_output, alpha=0.04, color='green')\n",
        "\n",
        "  mae = np.absolute(unscaled_output - unscaled_future)\n",
        "  mae = np.sum(mae)\n",
        "  total_mae += mae\n",
        "\n",
        "  mape = np.absolute((unscaled_output - unscaled_future) / unscaled_future)\n",
        "  mape = np.sum(mape)\n",
        "  total_mape += mape\n",
        "\n",
        "plt.show()\n",
        "\n",
        "total_mae /= n * horizon\n",
        "total_mape /= n * horizon\n",
        "\n",
        "print('Validation MAE:', total_mae)\n",
        "print('Validation MAPE:', total_mape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyE859YbjbRe"
      },
      "source": [
        "## Transferencia de aprendizaje para el comportamiento posterior al inicio de la cuarentena"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gewhDeHBCYgR"
      },
      "source": [
        "No contamos con muchos datos sobre el consumo de energía durante la cuarentena, por lo tanto no es factible entrenar un modelo directamente con estos datos.\n",
        "\n",
        "A la vez, el cambio en la dinámica previo y durante la cuarentena es muy drástico y un modelo entrenado con todos los datos muy probablemente tendría problemas para ajustarse correctamente.\n",
        "\n",
        "Por ello optamos por aplicar transferencia de aprendizaje. Esta técnica consta de partir de un modelo entrenado en una tarea y ajustarlo a una tarea distinta pero lo suficientemente similar para esperar que el modelo pueda generalizar correctamente. De esta forma podemos aprovechar todos los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgsTNmL6igGl"
      },
      "source": [
        "print('Data before COVID-19:', len(data_bc), sep='\\t')\n",
        "print('Data after COVID-19:', len(data_ac), sep='\\t')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVxxAAqkDmEv"
      },
      "source": [
        "Normalizamos los datos con el mismo escalador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622s0GTqeWqw"
      },
      "source": [
        "# Usar el mismo escalador\n",
        "data_ac_normalized = scaler.transform(data_ac)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cldTkRtgYbtM"
      },
      "source": [
        "def train_model(model, train_loader, epochs=3, lr=0.00001):\n",
        "  loss_function = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "  model.train(True)\n",
        "\n",
        "  for i in range(epochs):\n",
        "    print('Epoch:', i + 1)\n",
        "\n",
        "    # Train\n",
        "    print('Train')\n",
        "    for inpt, target in tqdm(train_loader):\n",
        "      model.init_hidden(inpt.size(0))\n",
        "      output = model(inpt.float()) \n",
        "      loss = loss_function(output, target.float())\n",
        "      loss.backward()\n",
        "      optimizer.step()  \n",
        "      optimizer.zero_grad() \n",
        "    print('Train loss :', loss.item())\n",
        "    print()\n",
        "  model.train(False)\n",
        "\n",
        "  return loss_function"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0J93NH5bvoD"
      },
      "source": [
        "def eval_model(model, test_loader, loss_function, scaler):\n",
        "  agg_preds = np.empty(len(test_loader))\n",
        "  print(len(test_loader))\n",
        "  preds = []\n",
        "  futures = []\n",
        "\n",
        "  running_loss = 0\n",
        "\n",
        "  for i, (inpt, target) in enumerate(tqdm(test_loader)):\n",
        "    unscaled_past = scaler.inverse_transform(inpt[0, :, :])[:, 0]\n",
        "\n",
        "    model.init_hidden(inpt.size(0))\n",
        "    output = model(inpt.float())\n",
        "    running_loss += loss_function(output, target.float()).item()\n",
        "    \n",
        "    output_array = np.zeros((horizon, n_features))\n",
        "    output_array[:, 0] = output.detach().numpy()\n",
        "    unscaled_output = scaler.inverse_transform(output_array)[:, 0]\n",
        "\n",
        "    real_future = np.zeros((horizon, n_features))\n",
        "    real_future[:, 0] = target\n",
        "    unscaled_future = scaler.inverse_transform(real_future)[:, 0]\n",
        "\n",
        "    preds.append(unscaled_output)\n",
        "    futures.append(unscaled_future)\n",
        "  \n",
        "  print('Test loss :', running_loss / len(test_loader))\n",
        "\n",
        "  return preds, futures\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKoGyapiDsAV"
      },
      "source": [
        "Una forma de evaluar el poder de generalización del modelo es utilizando validación cruzada.\n",
        "\n",
        "Para series de tiempo la validación cruzada consiste en dividir los datos en $k$ dobleces. Se parte de un determinado número de dobleces iniciales para entrenar a un modelo y se evalúa en el doblez siguiente. En cada iteración posterior se agrega un nuevo doblez para el entrenamiento y nuevamente se valida en el doblez siguiente.\n",
        "\n",
        "Al final todos los resultados de validación son agregados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Vg1YCto7Jf"
      },
      "source": [
        "k_folds = 10\n",
        "start_folds = 2\n",
        "cv_preds = dict()\n",
        "n_transfer_data = len(data_ac_normalized)\n",
        "n_fold_data = n_transfer_data // k_folds\n",
        "\n",
        "\n",
        "for i in range(start_folds, k_folds):\n",
        "  print(list(range(i)), i)\n",
        "  n_train_data = i * n_fold_data\n",
        "  train_data = data_ac_normalized[:n_train_data]\n",
        "  test_data = data_ac_normalized[n_train_data:n_train_data + n_fold_data]\n",
        "\n",
        "  train_dataset = TimeseriesDataset(train_data, train_data[:, 0], window, horizon)\n",
        "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test_dataset = TimeseriesDataset(test_data, test_data[:, 0], window, horizon)\n",
        "  test_loader = torch.utils.data.DataLoader(test_dataset)\n",
        "\n",
        "  # Clonar modelo base\n",
        "  model_i = LSTM(n_features, n_hidden, n_layers)\n",
        "  model_i.load_state_dict(model.state_dict())\n",
        "\n",
        "  loss_function = train_model(model_i, train_loader)\n",
        "  preds, futures = eval_model(model_i, test_loader, loss_function, scaler)\n",
        "  cv_preds[i] = (preds, futures)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXiK4XHw-xFY"
      },
      "source": [
        "Analizando el error en cada doblez podemos darnos una idea de qué tan dependiente es el modelo al número de datos disponibles de entrenamiento.\n",
        "\n",
        "Podemos esperar que conforme se agregan más dobleces el error de validación disminuirá. A la vez, si hay más cambios en el comportamiento veremos que el error aumente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbcLT8MzeAF-"
      },
      "source": [
        "n = 0\n",
        "total_mae = 0\n",
        "total_mape = 0\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(data_ac.to_numpy()[:,0])\n",
        "\n",
        "start_preds = start_folds * n_fold_data\n",
        "for fold, (preds, futures) in cv_preds.items():\n",
        "  for i, (pred, future) in enumerate(zip(preds, futures)):\n",
        "    start_range = start_preds + (fold - start_folds) * n_fold_data\n",
        "    x = np.arange(start_range + i, start_range + i + horizon)\n",
        "    plt.plot(x, pred, alpha=0.2, color='green')\n",
        "\n",
        "    mae = np.absolute(pred - future)\n",
        "    mae = np.sum(mae)\n",
        "    total_mae += mae\n",
        "\n",
        "    mape = np.absolute((pred - future) / future)\n",
        "    mape = np.sum(mape)\n",
        "    total_mape += mape\n",
        "\n",
        "    n += len(pred)\n",
        "\n",
        "  print(f'Validation MAE upto fold {fold}:', total_mae / n)\n",
        "  print(f'Validation MAPE upto fold {fold}:', total_mape / n)\n",
        "  print()\n",
        "  \n",
        "total_mae /= n\n",
        "total_mape /= n\n",
        "\n",
        "print('Validation MAE:', total_mae)\n",
        "print('Validation MAPE:', total_mape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}